import re
import json
import os
import logging
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from sentence_transformers import SentenceTransformer
import numpy as np

# Th∆∞ vi·ªán ƒë·ªçc t√†i li·ªáu
from PyPDF2 import PdfReader
from docx import Document as DocxDocument

# OCR h·ªó tr·ª£ fallback n·∫øu PDF l√† ·∫£nh
try:
    from pdf2image import convert_from_path
    import pytesseract
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False


class ZenPreprocessor:
    """
    L·ªõp ti·ªÅn x·ª≠ l√Ω (preprocessor) cho d·ªØ li·ªáu ZenCity Foundation:
    - L√†m s·∫°ch vƒÉn b·∫£n
    - Ph√¢n lo·∫°i t√†i li·ªáu gi√°o d·ª•c
    - Chunk vƒÉn b·∫£n
    - Sinh embeddings
    - L∆∞u JSONL/NPZ
    """

    def __init__(self, model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        """
        Kh·ªüi t·∫°o m√¥ h√¨nh SentenceTransformer (ƒëa ng√¥n ng·ªØ: Anh ‚Äì Vi·ªát)
        """
        self.model_name = model_name
        self.model = SentenceTransformer(model_name)

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s"
        )
        logging.info(f"‚úÖ ZenPreprocessor initialized with model: {model_name}")

    # ============================================================
    # 1Ô∏è‚É£ L√†m s·∫°ch vƒÉn b·∫£n
    # ============================================================
    def clean_text(self, text: str) -> str:
        """Lo·∫°i b·ªè HTML, script, k√Ω t·ª± ƒë·∫∑c bi·ªát, footer, sitemap,..."""
        if not text:
            return ""

        # X√≥a script/style + HTML
        text = re.sub(r'<(script|style).*?>.*?</\1>', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<.*?>', '', text)

        # Lo·∫°i b·ªè footer, banner, copyright, sitemap
        noise_patterns = [
            r'Follow us.*', r'Subscribe.*', r'Contact us.*',
            r'¬©.*\d{4}.*', r'Terms of Use.*', r'Privacy Policy.*',
            r'All rights reserved.*', r'Sitemap.*', r'Search.*'
        ]
        for pat in noise_patterns:
            text = re.sub(pat, '', text, flags=re.IGNORECASE)

        # Thay k√Ω t·ª± HTML ƒë·∫∑c bi·ªát
        html_entities = {
            '&nbsp;': ' ', '&amp;': '&', '&quot;': '"', '&apos;': "'",
            '&lt;': '<', '&gt;': '>', '\u2013': '-', '\u2014': '-',
            '\u2022': '‚Ä¢', '\u00a0': ' '
        }
        for k, v in html_entities.items():
            text = text.replace(k, v)

        # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    # ============================================================
    # 2Ô∏è‚É£ Ph√¢n lo·∫°i t√†i li·ªáu ZenCity
    # ============================================================
    def process_documents(self, documents):
        """L√†m s·∫°ch + ph√¢n lo·∫°i t√†i li·ªáu theo n·ªôi dung gi√°o d·ª•c ZenCity"""
        processed_docs = []

        for doc in documents:
            content = self.clean_text(doc.page_content)
            if not content:
                continue

            metadata = doc.metadata or {}
            doc_type = "general"

            # Ph√¢n lo·∫°i theo t·ª´ kh√≥a gi√°o d·ª•c
            if re.search(r"\b(course|class|lesson|ch∆∞∆°ng tr√¨nh|kh√≥a h·ªçc|b√†i h·ªçc)\b", content, re.IGNORECASE):
                doc_type = "course_material"
            elif re.search(r"\b(teacher|gi√°o vi√™n|training|ƒë√†o t·∫°o)\b", content, re.IGNORECASE):
                doc_type = "teacher_training"
            elif re.search(r"\b(student|h·ªçc vi√™n|tr·∫ª em|learner|h·ªçc sinh)\b", content, re.IGNORECASE):
                doc_type = "student_info"
            elif re.search(r"\b(event|s·ª± ki·ªán|workshop|seminar)\b", content, re.IGNORECASE):
                doc_type = "event_info"
            elif re.search(r"\b(blog|news|tin t·ª©c|b√†i vi·∫øt)\b", content, re.IGNORECASE):
                doc_type = "blog_post"
            elif re.search(r"\b(contact|li√™n h·ªá|hotline|email|ƒë·ªãa ch·ªâ)\b", content, re.IGNORECASE):
                doc_type = "contact_info"

            processed_doc = Document(
                page_content=content,
                metadata={**metadata, "type": doc_type}
            )
            processed_docs.append(processed_doc)

        logging.info(f"üßπ Processed {len(processed_docs)} documents.")
        return processed_docs

    # ============================================================
    # 3Ô∏è‚É£ Chia chunk vƒÉn b·∫£n
    # ============================================================
    def split_documents(self, documents, chunk_size=1000, chunk_overlap=200):
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ".", "!", "?", " "],
            length_function=len,
        )
        chunks = splitter.split_documents(documents)
        logging.info(f"üìÑ Split into {len(chunks)} chunks.")
        return chunks

    # ============================================================
    # 4Ô∏è‚É£ Sinh vector embeddings
    # ============================================================
    def embed_documents(self, documents):
        texts = [doc.page_content for doc in documents if doc.page_content.strip()]
        embeddings = self.model.encode(
            texts,
            show_progress_bar=True,
            batch_size=32,
            normalize_embeddings=True
        )
        logging.info(f"üß† Created {len(embeddings)} embeddings.")
        return embeddings

    # ============================================================
    # 5Ô∏è‚É£ L∆∞u d·ªØ li·ªáu
    # ============================================================
    def save_to_jsonl(self, documents, filename):
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        with open(filename, "w", encoding="utf-8") as f:
            for doc in documents:
                f.write(json.dumps({
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }, ensure_ascii=False) + "\n")
        logging.info(f"üíæ Saved {len(documents)} docs ‚Üí {filename}")

    def save_embeddings(self, embeddings, documents, out_path="data/zencity_embeddings.npz"):
        os.makedirs(os.path.dirname(out_path), exist_ok=True)
        np.savez_compressed(
            out_path,
            embeddings=embeddings,
            metadata=[doc.metadata for doc in documents]
        )
        logging.info(f"üíæ Saved embeddings ‚Üí {out_path}")

    # ============================================================
    # 6Ô∏è‚É£ Pipeline ch√≠nh: clean ‚Üí classify ‚Üí chunk
    # ============================================================
    def clean_and_chunk(self, raw_docs, chunk_size=1000, chunk_overlap=200):
        processed = self.process_documents(raw_docs)
        chunks = self.split_documents(processed, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        return chunks

    # ============================================================
    # 7Ô∏è‚É£ ƒê·ªçc PDF (text + OCR fallback)
    # ============================================================
    def read_pdf(self, file_path: str) -> str:
        text = ""
        try:
            reader = PdfReader(file_path)
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        except Exception as e:
            logging.warning(f"PDF read error: {e}")

        if not text.strip() and OCR_AVAILABLE:
            try:
                poppler_path = r"E:\Poppler\poppler-24.07.0\Library\bin"  # Update n·∫øu c·∫ßn
                images = convert_from_path(file_path, poppler_path=poppler_path)
                for img in images:
                    text += pytesseract.image_to_string(img, lang="vie+eng") + "\n"
            except Exception as e:
                logging.error(f"OCR failed for {file_path}: {e}")

        return self.clean_text(text)

    # ============================================================
    # 8Ô∏è‚É£ ƒê·ªçc TXT / DOCX
    # ============================================================
    def read_txt(self, file_path: str) -> str:
        with open(file_path, "r", encoding="utf-8") as f:
            return self.clean_text(f.read())

    def read_docx(self, file_path: str) -> str:
        doc = DocxDocument(file_path)
        text = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
        return self.clean_text(text)
