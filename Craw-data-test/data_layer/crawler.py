import requests
from bs4 import BeautifulSoup
import json
import re
import os
import logging
from urllib.parse import urljoin, urlparse
from typing import List, Set
import time

from langchain_community.document_loaders import WebBaseLoader
from langchain.schema import Document

# B·ªï sung Selenium ƒë·ªÉ gi·∫£ l·∫≠p tr√¨nh duy·ªát th·∫≠t (ch·ªëng ch·∫∑n bot)
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

# Tenacity ƒë·ªÉ retry t·ª± ƒë·ªông khi t·∫£i HTML fail
from tenacity import retry, stop_after_attempt, wait_exponential

# Headers gi·∫£ l·∫≠p tr√¨nh duy·ªát Chrome (gi√∫p tr√°nh b·ªã ch·∫∑n 403 Forbidden)
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/115.0.0.0 Safari/537.36"
    ),
    "Accept": (
        "text/html,application/xhtml+xml,application/xml;"
        "q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8"
    ),
    "Accept-Language": "en-US,en;q=0.9,vi;q=0.8",
}

# =============================================================
# C·∫•u h√¨nh logging
# =============================================================
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# =============================================================
# H√ÄM T·∫¢I HTML V·ªöI C∆† CH·∫æ TH·ª¨ L·∫†I (retry)
# =============================================================
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, max=10))
def fetch_html(url: str) -> str:
    """T·∫£i HTML t·ª´ URL, c√≥ retry n·∫øu l·ªói m·∫°ng ho·∫∑c 403"""
    resp = requests.get(url, headers=HEADERS, timeout=15)
    resp.raise_for_status()
    return resp.text

# =============================================================
# H√ÄM L√ÄM S·∫†CH HTML
# =============================================================
def clean_html_text(html: str) -> str:
    """Lo·∫°i b·ªè script, style, footer, header, nav,... v√† chu·∫©n h√≥a text"""
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "nav", "footer", "header", "aside", "form", "iframe"]):
        tag.decompose()
    text = soup.get_text(separator=" ", strip=True)
    text = re.sub(r'\s+', ' ', text)

    # Chu·∫©n h√≥a k√Ω t·ª± HTML ƒë·∫∑c bi·ªát
    html_entities = {
        '&nbsp;': ' ', '&amp;': '&', '&quot;': '"',
        '&apos;': "'", '&lt;': '<', '&gt;': '>',
        '\u2013': '-', '\u2014': '-', '\u2022': '‚Ä¢',
    }
    for k, v in html_entities.items():
        text = text.replace(k, v)
    return text

# =============================================================
# H√ÄM CRAWL B·∫∞NG REQUESTS + BEAUTIFULSOUP
# =============================================================
def crawl_url(url: str) -> List[Document]:
    """T·∫£i n·ªôi dung trang web b·∫±ng requests + BeautifulSoup"""
    try:
        html = fetch_html(url)
        text = clean_html_text(html)
        return [Document(page_content=text, metadata={"url": url})]
    except Exception as e:
        logging.warning(f"Crawl requests+BS4 th·∫•t b·∫°i {url}: {e}")
        return []

# =============================================================
# L·ªöP CRAWL B·∫∞NG SELENIUM
# =============================================================
class SeleniumBrowser:
    """Gi·ªØ Selenium driver m·ªü ƒë·ªÉ crawl nhi·ªÅu URL li√™n ti·∫øp"""
    def __init__(self):
        try:
            options = Options()
            options.add_argument("--headless=new")
            options.add_argument(f"user-agent={HEADERS['User-Agent']}")
            options.add_argument("--disable-blink-features=AutomationControlled")
            self.driver = webdriver.Chrome(options=options)
        except Exception as e:
            logging.error(f"Kh√¥ng th·ªÉ kh·ªüi t·∫°o ChromeDriver: {e}")
            self.driver = None

    def crawl(self, url: str) -> List[Document]:
        if not self.driver:
            return []
        """T·∫£i d·ªØ li·ªáu b·∫±ng Selenium (gi·∫£ l·∫≠p browser th·∫≠t)"""
        try:
            self.driver.get(url)
            html = self.driver.page_source
            text = clean_html_text(html)
            return [Document(page_content=text, metadata={"url": url})]
        except Exception as e:
            logging.warning(f"Selenium crawl th·∫•t b·∫°i {url}: {e}")
            return []

    def close(self):
        """ƒê√≥ng tr√¨nh duy·ªát Selenium"""
        self.driver.quit()

# =============================================================
# H√ÄM CRAWL AN TO√ÄN (TH·ª¨ NHI·ªÄU C√ÅCH)
# =============================================================
def safe_load_url(url: str):
    """
    T·∫£i d·ªØ li·ªáu an to√†n:
    - Th·ª≠ WebBaseLoader c·ªßa LangChain tr∆∞·ªõc
    - N·∫øu fail, d√πng requests + BeautifulSoup
    - N·∫øu v·∫´n fail, fallback sang Selenium
    """
    try:
        loader = WebBaseLoader(url)
        docs = loader.load()
        logging.info(f"WebBaseLoader loaded {len(docs)} docs from {url}")
        return docs
    except Exception as e:
        logging.warning(f"WebBaseLoader failed for {url}: {e} ‚Üí th·ª≠ requests+BS4")
        docs = crawl_url(url)
        if docs:
            return docs
        logging.warning(f"requests+BS4 c≈©ng fail ‚Üí th·ª≠ Selenium")
        browser = SeleniumBrowser()
        docs = browser.crawl(url)
        browser.close()
        return docs

# =============================================================
# L·ªöP CH√çNH: ZenCrawler ‚Äî Thu th·∫≠p d·ªØ li·ªáu ZenCity
# =============================================================
class ZenCrawler:
    """Crawler chuy√™n d·ª•ng cho ZenCity (gi√°o d·ª•c)"""
    def __init__(self):
        # C√°c ngu·ªìn ch√≠nh c·ªßa ZenCity
        self.sources = {
            "zencity_home": "https://www.zencityfoundation.org/",  # Trang ch√≠nh gi·ªõi thi·ªáu t·ªï ch·ª©c :contentReference[oaicite:1]{index=1}
            "zencity_programs": "https://www.zencityfoundation.org/our-programs",  # Trang c√°c ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o :contentReference[oaicite:2]{index=2}
            "zencity_learn_vietnamese": "https://www.zencityfoundation.org/vi/vietnamese",  # Ch∆∞∆°ng tr√¨nh h·ªçc ti·∫øng Vi·ªát :contentReference[oaicite:3]{index=3}
            "zencity_learn_english": "https://www.zencityfoundation.org/vi/learn-english",  # Ch∆∞∆°ng tr√¨nh h·ªçc ti·∫øng Anh :contentReference[oaicite:4]{index=4}
            "zencity_teacher_training": "https://www.zencityfoundation.org/online-teacher-training",  # Kh√≥a ƒë√†o t·∫°o gi√°o vi√™n online :contentReference[oaicite:5]{index=5}
            "zencity_accent_training": "https://www.zencityfoundation.org/accent-training",  # Kh√≥a luy·ªán ph√°t √¢m ti·∫øng Anh (Accent training) :contentReference[oaicite:6]{index=6}
            "zencity_certificates": "https://www.zencityfoundation.org/certificate",  # M·ª•c ch·ª©ng ch·ªâ & ch∆∞∆°ng tr√¨nh li√™n quan :contentReference[oaicite:7]{index=7}
            "zencity_events": "https://www.zencityfoundation.org/event-list",  # Trang s·ª± ki·ªán c·ªßa t·ªï ch·ª©c :contentReference[oaicite:8]{index=8}
            "zencity_blog_post_english_for_workers": "https://www.zencityfoundation.org/post/kham-pha-khoa-hoc-tieng-anh-danh-cho-nguoi-di-lam-tai-zen-city-foundation",  # V√≠ d·ª• b√†i vi·∫øt blog ‚Äé:contentReference[oaicite:9]{index=9}
        }
        self.crawled_urls: Set[str] = set()

    # =============================================================
    # L·∫§Y T·∫§T C·∫¢ LINK CON TRONG 1 TRANG CH√çNH
    # =============================================================
    def get_all_links(self, base_url: str, limit: int = 20) -> List[str]:
        """L·∫•y to√†n b·ªô link con trong c√πng domain (gi·ªõi h·∫°n limit)"""
        try:
            html = fetch_html(base_url)
        except Exception as e:
            logging.error(f"Error fetching {base_url}: {e}")
            return []

        soup = BeautifulSoup(html, "html.parser")
        base_domain = urlparse(base_url).netloc
        links: Set[str] = set()

        for a in soup.find_all("a", href=True):
            href = a["href"]
            full_url = urljoin(base_url, href)
            # Ch·ªâ l·∫•y link trong c√πng domain
            if urlparse(full_url).netloc == base_domain:
                links.add(full_url)

        links = list(links)[:limit]
        logging.info(f"üîó Found {len(links)} links in {base_url}")
        return links

    # =============================================================
    # CRAWL TO√ÄN B·ªò DOMAIN
    # =============================================================
    def crawl_domain(self, source_name: str, base_url: str, limit: int = 20) -> List[Document]:
        """
        Crawl to√†n b·ªô link con trong 1 domain:
        - Th√™m metadata: source, url, length
        - Tr√°nh crawl tr√πng l·∫∑p
        """
        docs = []
        urls = self.get_all_links(base_url, limit=limit)
        if base_url not in urls:
            urls.insert(0, base_url)

        for url in urls:
            if url in self.crawled_urls:
                continue
            try:
                loaded_docs = safe_load_url(url)  # D√πng safe_load_url thay v√¨ crawl tr·ª±c ti·∫øp
                for doc in loaded_docs:
                    doc.metadata["source"] = source_name
                    doc.metadata["url"] = url
                    doc.metadata["length"] = len(doc.page_content.split())
                docs.extend(loaded_docs)
                self.crawled_urls.add(url)
                logging.info(f"Crawled {len(loaded_docs)} docs from {url}")
            except Exception as e:
                logging.warning(f"Skipped {url}: {e}")
        return docs

    # =============================================================
    # L∆ØU T√ÄI LI·ªÜU RA FILE JSONL
    # =============================================================
    def save_documents_to_jsonl(self, docs: List[Document], filename: str):
        """L∆∞u danh s√°ch documents ra file JSONL"""
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        with open(filename, "w", encoding="utf-8") as f:
            for doc in docs:
                f.write(json.dumps({
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }, ensure_ascii=False) + "\n")
        logging.info(f"Saved {len(docs)} documents to {filename}")
